
# [Algorithm Analysis & Design] Assignment
## Week - 5, Lecture - 2

## Dynamic Programming
 - Dynamic Programming is mainly an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls for same inputs, we can optimize it using Dynamic Programming. 
 - The idea is to simply store the results of subproblems, so that we do not have to re-compute them when needed later. This simple optimization reduces time complexities from exponential to polynomial.
 - DP algorithm solves each subproblem just once and then remembers its answer, thereby avoiding re-computation of the answer for similar subproblem every time.


### There are two ways of doing this: 
####    Top-Down
- Start solving the given problem by breaking it down. If you see that the problem has been solved already, then just return the saved answer. If it has not been solved, solve it and save the answer. This is referred to as Memoization.
####    Bottum-Up
- Analyze the problem and see the order in which the sub-problems are solved and start solving from the trivial subproblem, up towards the given problem. In this process, it is guaranteed that the subproblems are solved before solving the problem. This is referred to as Dynamic Programming.

### 1. Shortest Path in Directed Acyclic Graph and  Topological Sort
   In the Directed Acyclic Graph, Topological sort is a way of the linear ordering of vertices v1, v2, …. vN in such a way that for every directed edge x → y, x will come before y in the ordering.
 
#### Algorithm

    1. Call DFS to compute finish time f[v] for each vertex.
    2. As each vertex is finished, insert it onto the front of list.
    3. Return the list of vertices.

#### Pseudocode
``` bash 
Input: A DAG G
Output: A list of the vertices of G in topological order

Topological-Sort(Graph G)
     S = Stack()
     L = List()

     for each vertex in G:
        if vertex has no incident edge:
            S.push(vertex)

        while S is not empty:
             v = S.pop()
             L.append(v)
             for each outgoing edge e from v:
                w = e.destination
                delete e
                if w has no incident edges:
                    S.push(w)
 return L
```
Time Complexity : O(|V| + |E|).

#### * Distance(far nodes) From Distance(near nodes)
``` bash
initialize all dist values to INF
dist(s) = 0
for each v ε V\{s}, in linearized order:
    dist(v) = min{dist(u) + l(u, v)} where (u, v) ε E
```
### 2. Longest Increasing Subsequence
 Input:  Given 2 sequences of elements S1 and S2, a subsequence of it can be obtained by removing zero or more elements from the sequence, preserving the relative order of the elements.
 
 Output: To find the the length of the longest subsequence that is common to the given two Strings S1 and S2.   

#### 1.  Algorithm 1 (Using Recursion) 
#### Pseudocode
```bash
The recursive relation is as follows:

LIS(i) = 1 + max( LIS(j) ) where 0 < j < i and arr[j] < arr[i]; or
LIS(i) = 1, if no such j exists.

```
#### Time Complexity:
Time complexity of this recursive approach is exponential i.e. O(2^n).
 #### 2. Algorithm 2 (Dynamic Programming)
There are many subproblems in the above recursive solution which are solved again and again. So this problem has Overlapping Substructure property and recomputation of same subproblems can be avoided by either using Memoization or Tabulation.

#### Implementation

``` bash
Longest-Increasing-Subsequence(arr, n)
{
    int seq[n];
    seq[0] = 1;
    
    for (int i = 1; i < n; i++) {
        seq[i] = 1;
        for (int j = 0; j < i; j++)
            if (arr[i] > arr[j] && seq[i] < seq[j] + 1)
                seq[i] = seq[j] + 1;
    }

    int max = INF
    for (int i = 0; i < n; i++)
        if (max < seq[i])
            max = seq[i];
    
    return max;
}
```
#### Time Complexity:
 - Time Complexity of this approach is: O(n^2).
 - Auxiliary Space: O(n).